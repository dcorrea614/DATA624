---
title: "DATA624 - Project 2"
author: "Diego Correa & Lisa"
date: e
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true"December 02, 2021"
always_allow_html: yes
output:
  html_document:
    df_print: kabl
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(psych)
library(caret)
library(mice)
library(randomForest)
library(caTools)
library(corrplot)
library(class)
library(rpart)
library(AppliedPredictiveModeling)
library(naniar)
library(xgboost)
library(DiagrammeR)
library(readxl)
library(DataExplorer)
library(elasticnet)
library(glmnet)
library(caTools)
library(rattle)
```

## Background

### Data Dictionary

### Problem Statement


## Dataset

```{r warning=FALSE, message=FALSE}
df <- read_xlsx('C:\\Users\\dcorr\\Dropbox\\MSDS\\DATA624\\Project2\\StudentData.xlsx')
head(df)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="300px")

str(df)
```

### Descriptive Dataset Summary

```{r warning=FALSE, message=FALSE}
summary(df)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

## Pre-Processing

### Missing Value Analysis

Based on the above descriptive data summary, there are quite a few variables with missing values. So we conducted an analysis of all missing values in various attributes to identify proper imputation technique.

```{r fig.height=4, message=FALSE, warning=FALSE}
## Counts of missing data per feature
dataset_missing_counts <- data.frame(apply(df, 2, function(x) length(which(is.na(x)))))
dataset_missing_pct <- data.frame(apply(df, 2,function(x) {sum(is.na(x)) / length(x) * 100}))

dataset_missing_counts <- cbind(Feature = rownames(dataset_missing_counts), dataset_missing_counts, dataset_missing_pct)
colnames(dataset_missing_counts) <- c('Feature','NA_Count','NA_Percentage')
rownames(dataset_missing_counts) <- NULL

dataset_missing_counts <- dataset_missing_counts %>% filter(`NA_Count` != 0) %>% arrange(desc(`NA_Count`))

dataset_missing_counts  %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  geom_label(aes(label = NA_Count)) +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())
```


Before performing the imputation, we need to change the categorical variable into a factor and clean up whitespaces from column names.

```{r}
# manipulation
df1 <- df %>%
  mutate(`Brand Code` = as.factor(`Brand Code`))

# cleaning up the column names for the imputation function
colNamesNoSpace <- colnames(df1) %>%
  str_remove_all(' ')

colnames(df1) <- colNamesNoSpace
```

### Degenerate Variables

We also checked for presence of any de-generate variables.  Here, we see that there is one degenerate variable.

```{r}
# near zero variance
# capturing the degenerate variables
dim(df1)
nzv <- nearZeroVar(df1)

# identifying them 
(remove<-colnames(df1)[nzv])

# removing from the dataset

df2 <- df1[,!(colnames(df1) == remove)]
dim(df2)
```


### Removing Rows with Missing Brand Code


```{r}
(countNA<-colSums(is.na(df2)))

#remove missing brand =>df3
df3 <- df2[!is.na(df2$BrandCode), ]
dim(df3)

# DC - not sure if we want to include graphs in this section?

histogram(df3$MFR)
plot(df3$MFR,df3$PH)


table(df3$BrandCode)
boxplot(df3$PH~df3$BrandCode , main="PH by Brand",ylab="PH",  xlab="",las=2)

boxplot(df3$MFR~df3$BrandCode , main="MFR by Brand",ylab="MRF",  xlab="",las=2)

dim(df3)

```


### Finding Highly Correlated Variables

None appear to be highly correlated

```{r warning=FALSE}
# remove highly correlated variables
corrMatrix <- round(cor(df3[,-1]),4)

highCorr <- findCorrelation(corrMatrix,
  cutoff = 0.9,
  verbose = FALSE,
  names = TRUE,
  exact = TRUE)

# identify
highCorr %>% 
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")


# remove from dataset
df4 <- df3
dim(df4)
```

### Data Imputation


Once done, we can perform the imputation using the *random forest* method of the *mice* package

```{r message=FALSE, warning=FALSE}
#imputation by using the random forest method ('rf')
init <- mice(df3, maxit = 0)
predM <- init$predictorMatrix
set.seed(123)
imputed <- mice(df3, method = 'rf', predictorMatrix = predM, m=1, silent = TRUE)
```


```{r message=FALSE, warning=FALSE}
df3 <- complete(imputed, silent = TRUE)
summary(df3)
```



## Exploratory Analysis


### Distribution

```{r}
# DC - i think we can remove this as we are adding a boxplt graph below
boxplot(df4$PH ~df4$BrandCode, main="PH by BrandCode", ylab="
        ph", cex.axis=.5, xlab="",las=2) 

DataExplorer::plot_histogram(df3)

# make dataset long to place data in boxplots
vars <- df4 %>%
  select(-BrandCode) %>%
  gather(key = 'variables', value = 'value') 

# boxplot
vars %>%
  ggplot(aes(x = variables, y = value)) +
  geom_boxplot() +
  labs(title = 'Distributions of Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()

# DC - i don't think we need to look at separately if we are using the plot_histogram function, right?

#look at separately
histogram(df4$CarbVolume)
histogram(df4$FillOunces)
histogram(df4$PCVolume)
histogram(df4$CarbPressure)
histogram(df4$CarbTemp)
histogram(df4$PSC)
histogram(df4$PSCFill)
histogram(df4$PSCCO2)
histogram(df4$MnfFlow)
histogram(df4$CarbPressure1)
histogram(df4$FillPressure)
histogram(df4$HydPressure2)
histogram(df4$HydPressure3)
histogram(df4$HydPressure4)
histogram(df4$FillerLevel)
histogram(df4$FillerSpeed)
histogram(df4$Temperature)
histogram(df4$Usagecont)
histogram(df4$CarbFlow)
histogram(df4$Density)
histogram(df4$MFR)
histogram(df4$Balling)
histogram(df4$PressureVacuum)
histogram(df4$PH)
histogram(df4$OxygenFiller)
histogram(df4$BowlSetpoint)
histogram(df4$PressureSetpoint)
histogram(df4$AirPressurer)
histogram(df4$AlchRel)
histogram(df4$CarbRel)
histogram(df4$BallingLvl)

```

### Correlation Plot: Multicollinearity Check


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
corrMatrix <- round(cor(df4[,-1]),4)
corrMatrix %>% corrplot(., method = "color", outline = T, addgrid.col = "darkgray", order="hclust", addrect = 4, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.0, cl.cex = 1.0, addCoef.col = "white", number.digits = 2, number.cex = 0.8, col = colorRampPalette(c("darkred","white","dodgerblue4"))(100))
```



## Model Building

### Overview




### Splitting Data: Train/Test

```{r}
set.seed(123)
sample <- sample.split(df$PH, SplitRatio = 0.75)
train <- subset(df, sample == TRUE)
train <- model.matrix( ~ .-1, train)

test <- subset(df, sample == FALSE)
test <- model.matrix( ~ .-1, test)

y_train <- train[,'PH']
y_test <- test[,'PH']

X_train <- train[, !(colnames(train) == 'PH')]
X_test <- test[, !(colnames(test) == 'PH')]
```



### Linear Regression 

### Ordinary Least Square

```{r}
lmFit <- train(X_train, y_train,
                  method = 'lm',
                  preProc = c('center','scale'),
                  trControl = trainControl(method = 'cv')
                  )
lmFit
```



```{r}
varimp <- varImp(lmFit)

varimp$importance %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


```{r}
lm_Pred <- predict(lmFit, newdata = X_test)
postResample(pred = lm_Pred, obs = y_test)
```



#### Elastic Net



```{r}
#Diego
enetGrid <- expand.grid(.lambda = c(0,0.01,0.1),
            .fraction = seq(0.05,1,length = 20))

set.seed(213)
enetTune <- train(X_train, y_train,
                  method = 'enet',
                  preProc = c('center','scale'),
                  tuneGrid = enetGrid,
                  trControl = trainControl(method = 'cv')
                  )
enetTune
```



```{r}
varimp <- varImp(enetTune)

varimp$importance %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


Let’s look at new samples…

```{r}
enetTune <- predict(enetTune, newdata = X_test)
postResample(pred = enetTune, obs = y_test)
```




### Non Linear Regression 

#### Neural Network

```{r}
# neural network
nnetGrid <- expand.grid(.decay = c(0,0.01,.1),
                        .size = c(1:5),
                        .bag = FALSE)

nnetFit <- train(X_train, y_train,
                  method = 'avNNet',
                  preProc = c('center','scale'),
                  tuneGrid = nnetGrid,
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 5 * (ncol(X_train) + 1 + 5 + 1),
                  maxit = 100
  
)
nnetFit
```

#### DISCUSSION Neural Network: 

Results:

a 10-5-1 network with 61 weights

Let’s look at variable importance…


```{r}
varimp <- varImp(nnetFit)

varimp$importance %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


Let’s look at new samples…..
```{r}
set.seed(516)
nnetpred <- predict(nnetFit, newdata = X_test)
postResample(pred = nnetpred, obs = y_test)
```

The neural network RMSE =0.11452281,
Rsquare= 0.54636835, 
MAE= 0.08871147 on test data.

### Trees and Boosting

#### Random Forest

```{r}
rfmodel <- train(X_train, y_train,
                 method = 'rf',
                 preProc = c('center','scale'),
                 trControl = trainControl(method = 'cv'))

rfmodel
```



```{r}
varimp <- varImp(rfmodel)

varimp$importance %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


Let’s look at new samples…..


```{r}
rfmodel <- predict(rfmodel, newdata = X_test)
postResample(pred = rfmodel, obs = y_test)
```
The random Forest RMSE =0.08944819,
Rsquare= 0.73302854, 
MAE= 0.06709861 on test data.


#### Cubist


```{r}
#CUBIST
set.seed(123)

cubist_Model <- train(x = X_train, y = y_train,
                      method = "cubist",
                      preProc = c('center','scale'),
                      trControl = trainControl(method = 'cv'))

cubist_Model
```

What are the important variables of the CUBIST model?

```{r}
varImp(cubist_Model)
```


Let’s look at new samples…

```{r}
cubist_Pred <- predict(cubist_Model, newdata = X_test)
postResample(pred = cubist_Pred, obs = y_test)
```
Test Data..
The cubist RMSE =0.09161601,
Rsquare= 0.71007579, 
MAE= 0.06955146.





#### Model Summary


```{r}

```



### Model Summary

```{r}

```


#### Model Summary

```{r}

```


### Conclusion

```{r}

```


