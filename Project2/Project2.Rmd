---
title: "DATA624 - Project 2"
author: "Diego Correa & Lisa"
date: "December 02, 2021"
always_allow_html: yes
output:
  html_document:
    df_print: kable
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(psych)
library(caret)
library(mice)
library(randomForest)
library(caTools)
library(corrplot)
library(class)
library(rpart)
library(AppliedPredictiveModeling)
library(naniar)
library(xgboost)
library(DiagrammeR)
library(readxl)
library(writexl)
library(DataExplorer)
library(elasticnet)
library(glmnet)
library(rattle)
```

## Background

### Data Dictionary

### Problem Statement


## Dataset

```{r warning=FALSE, message=FALSE}
df <- read_xlsx('StudentData.xlsx')
head(df)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="300px")
```

### Descriptive Dataset Summary

```{r warning=FALSE, message=FALSE}
summary(df)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

## Pre-Processing




Before performing the imputation, we need to change the categorical variable into a factor and clean up whitespaces from column names.

```{r}
# transforming the Brand Code variable into a factor for imputation
df <- df %>%
  mutate(`Brand Code` = as.factor(`Brand Code`))

# cleaning up the column names for the imputation function
colNamesNoSpace <- colnames(df) %>%
  str_remove_all(' ')

# transforming the column names
colnames(df) <- colNamesNoSpace
```

### Degenerate Variables

We also checked for presence of any de-generate variables.  Here, we see that there is one degenerate variable, *HydPressure1*.

```{r}
# near zero variance
# capturing the degenerate variables
dim(df)
nzv <- nearZeroVar(df)

# identifying them 
(remove<-colnames(df)[nzv])

# removing from the dataset
df <- df[,!(colnames(df) == remove)]
dim(df)
```

### Missing Value Analysis

Based on the above descriptive data summary, there are quite a few variables with missing values. So we conducted an analysis of all missing values in various attributes to identify proper imputation technique.

```{r fig.height=4, message=FALSE, warning=FALSE}
## Counts of missing data per feature
dataset_missing_counts <- data.frame(apply(df, 2, function(x) length(which(is.na(x)))))
dataset_missing_pct <- data.frame(apply(df, 2,function(x) {sum(is.na(x)) / length(x) * 100}))

dataset_missing_counts <- cbind(Feature = rownames(dataset_missing_counts), dataset_missing_counts, dataset_missing_pct)
colnames(dataset_missing_counts) <- c('Feature','NA_Count','NA_Percentage')
rownames(dataset_missing_counts) <- NULL

dataset_missing_counts <- dataset_missing_counts %>% filter(`NA_Count` != 0) %>% arrange(desc(`NA_Count`))

dataset_missing_counts  %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  geom_label(aes(label = NA_Count)) +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())
```

### Removing Rows with Missing Brand Code

To avoid risk of imputing the incorrect Brand Code, we remove the *NA* rows, a total 120 rows.

```{r}
#remove missing brand => df3
df <- df[!is.na(df$BrandCode), ]
dim(df)
```


### Finding Highly Correlated Variables

None appear to be highly correlated

```{r warning=FALSE}
# remove highly correlated variables
corrMatrix <- round(cor(df[,-1]),4)

highCorr <- findCorrelation(corrMatrix,
  cutoff = 0.9,
  verbose = FALSE,
  names = TRUE,
  exact = TRUE)

# identify
highCorr %>% 
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="200px")
```

### Data Imputation


Once done, we can perform the imputation using the *random forest* method of the *mice* package

```{r message=FALSE, warning=FALSE}
#imputation by using the random forest method ('rf')
init <- mice(df, maxit = 0)
predM <- init$predictorMatrix
set.seed(123)
imputed <- mice(df, method = 'rf', predictorMatrix = predM, m=1, silent = TRUE)
```


```{r message=FALSE, warning=FALSE}
df <- complete(imputed, silent = TRUE)
summary(df)
```



## Exploratory Analysis


### Distribution

```{r}
# Categorical
boxplot(df$PH ~df$BrandCode, main="PH by BrandCode", ylab="
        ph", cex.axis=.5, xlab="",las=2) 

# numerical
# make dataset long to place data in boxplots
vars <- df %>%
  select(-BrandCode) %>%
  gather(key = 'variables', value = 'value') 

# boxplot
vars %>%
  ggplot(aes(x = variables, y = value)) +
  geom_boxplot() +
  labs(title = 'Boxplot of Numerical Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()

# histogram of variables
DataExplorer::plot_histogram(df)
```

### Correlation Plot: Multicollinearity Check


```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
corrMatrix <- round(cor(df[,-1]),4)

corrMatrix %>% corrplot(., method = "color", outline = T, addgrid.col = "darkgray", order="hclust", addrect = 4, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.0, cl.cex = 1.0, addCoef.col = "white", number.digits = 2, number.cex = 0.5, col = colorRampPalette(c("darkred","white","dodgerblue4"))(100))
```


## Model Building

### Overview


### Splitting Data: Train/Test

```{r}
set.seed(123)
sample <- sample.split(df$PH, SplitRatio = 0.75)
train <- subset(df, sample == TRUE)
train <- model.matrix( ~ .-1, train)

test <- subset(df, sample == FALSE)
test <- model.matrix( ~ .-1, test)

y_train <- train[,'PH']
y_test <- test[,'PH']

X_train <- train[, !(colnames(train) == 'PH')]
X_test <- test[, !(colnames(test) == 'PH')]
```



### Linear Regression 

#### Ordinary Least Square

```{r warning=FALSE}
lmFit <- train(X_train, y_train,
                  method = 'lm',
                  preProc = c('center','scale'),
                  trControl = trainControl(method = 'cv')
                  )
lmFit
```

##### Residual Analysis

```{r warning=FALSE}
res <- resid(lmFit)
predVal <- predict(lmFit)
plot(y_train, res)
abline(0,0)
plot(y_train,predVal)
```


##### Variable Importance

```{r}
varimp <- varImp(lmFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

##### Prediction Performance

```{r warning=FALSE}
lm_Pred <- predict(lmFit, newdata = X_test)
lmPredPerf <- postResample(pred = lm_Pred, obs = y_test)

lmPredPerf['Family'] <- 'Linear'

# saving model prediction performance
modelPredPerf <- data.frame(rbind(lm = lmPredPerf))
```



#### Elastic Net


```{r}
enetGrid <- expand.grid(.lambda = c(0,0.01,0.1),
            .fraction = seq(0.05,1,length = 20))

set.seed(213)
enetTune <- train(X_train, y_train,
                  method = 'enet',
                  preProc = c('center','scale'),
                  tuneGrid = enetGrid,
                  trControl = trainControl(method = 'cv')
                  )
enetTune
```

##### Variable Importance

```{r}
varimp <- varImp(enetTune)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


##### Prediction Performance

```{r}
enetPred <- predict(enetTune, newdata = X_test)
enetPredPerf <- postResample(pred = enetPred, obs = y_test)

enetPredPerf['Family'] <- 'Linear'

# saving model prediction performance
modelPredPerf <- rbind(modelPredPerf,elastic_net = enetPredPerf)
```




### Non Linear Regression 

#### Support Vector Machine

```{r}
svmRTuned <- train(X_train, y_train,
                 method = 'svmRadial',
                 preProc = c('center','scale'),
                 tuneLength = 14,
                 trControl = trainControl(method = 'cv'))

svmRTuned
```

##### Variable Importance

```{r}
varimp <- varImp(svmRTuned)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


##### Prediction Performace 

```{r}
svmRPred <- predict(svmRTuned, newdata = X_test)
svmRPredPerf <- postResample(pred = svmRPred, obs = y_test)

svmRPredPerf['Family'] <- 'NonLinear'

# saving model prediction performance
modelPredPerf <- rbind(modelPredPerf,svm_radial = svmRPredPerf)

```



#### Neural Network

```{r warning=FALSE}
# neural network
nnetGrid <- expand.grid(.decay = c(0,0.01,.1),
                        .size = c(1:5),
                        .bag = FALSE)

nnetFit <- train(X_train, y_train,
                  method = 'avNNet',
                  preProc = c('center','scale'),
                  tuneGrid = nnetGrid,
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 5 * (ncol(X_train) + 1 + 5 + 1),
                  maxit = 100
  
)
nnetFit
```


##### Variable Importance


```{r}
varimp <- varImp(nnetFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


##### Prediction Performance

```{r}
nnetPred <- predict(nnetFit, newdata = X_test)
nnetPredPerf <- postResample(pred = nnetPred, obs = y_test)

nnetPredPerf['Family'] <- 'NonLinear'

# saving model prediction performance
modelPredPerf <- rbind(modelPredPerf,neural_net = nnetPredPerf)
```

The neural network RMSE =0.11452281,
Rsquare= 0.54636835, 
MAE= 0.08871147 on test data.

### Trees and Rules

#### Random Forest

```{r}
rfmodel <- train(X_train, y_train,
                 method = 'rf',
                 preProc = c('center','scale'),
                 trControl = trainControl(method = 'cv'))

rfmodel
```

##### Variable Importance

```{r}
varimp <- varImp(rfmodel)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


##### Prediction Performance



```{r}
rfPred <- predict(rfmodel, newdata = X_test)
rfPredPerf <- postResample(pred = rfPred, obs = y_test)


rfPredPerf['Family'] <- 'TreesAndRules'

# saving model prediction performance
modelPredPerf <- rbind(modelPredPerf, rf = rfPredPerf)
```



#### Cubist


```{r}
set.seed(123)

cubist_Model <- train(x = X_train, y = y_train,
                      method = "cubist",
                      preProc = c('center','scale'),
                      trControl = trainControl(method = 'cv'))

cubist_Model
```

##### Prediction Performance

```{r}
cubistPred <- predict(cubist_Model, newdata = X_test)
cubistPredPerf <- postResample(pred = cubistPred, obs = y_test)


cubistPredPerf['Family'] <- 'TreesAndRules'

# saving model prediction performance
modelPredPerf <- rbind(modelPredPerf,cubist = cubistPredPerf)
```

### Evaluation

Comparing the *Rqaured*, *RSME*, and *MAE* metric of each model's prediction on the hold out data.  We will use the the model with the highest *Rsquared* value as this will explains the most about the variance of the predicted value.  

Here, we see that the *Trees and Rules* models outperforms the *Non Linear* and *Linear* regression models.

```{r}
modelPredPerf %>%
  mutate(RMSE = round(as.numeric(RMSE),3),
         Rsquared = round(as.numeric(Rsquared),3),
         MAE = round(as.numeric(MAE),3),) %>%
  arrange(desc(Rsquared)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```




## Conclusion





## Prediction

Lastly, we will use our best performing model, Random Forest, on the evaluation dataset.  In order to use the mdoel, we need to perform the transformations that we used on our dataset.

```{r}
# reading the evaluation dataset
eval_df <- read_xlsx('StudentEvaluation.xlsx')

# change Brand Code into a factor
eval_df <- eval_df %>%
  mutate(`Brand Code` = as.factor(`Brand Code`)) 

# remove white spaces in column names
colNamesNoSpace <- colnames(eval_df) %>%
  str_remove_all(' ')

colnames(eval_df) <- colNamesNoSpace

# remove degenerate variable
eval_df <- eval_df[,!(colnames(eval_df) == remove)]

# looking at NA values
(countNA <- colSums(is.na(eval_df)))
```


We can see that the dataset has missing values.  The generated *Random Forest* model needs values in each entry to make a prediction on the entry.  Thus, imputation is needed prior to making predictions.  We will use the same technique as before.


```{r message=FALSE, warning=FALSE}
#imputation by using the random forest method ('rf')
init <- mice(eval_df, maxit = 0)
predM <- init$predictorMatrix
set.seed(123)
imputed <- mice(eval_df, method = 'rf', predictorMatrix = predM, m=1, silent = TRUE)
eval_df <- complete(imputed, silent = TRUE)
```


Next, we need to drop the *PH* in the evaluation dataset to transform it into a matrix model that includes dummy variables for the factor data type.

```{r}
# remove PH variable
eval_df2 <- eval_df[,!(colnames(eval_df) == 'PH')]

# create model matrix
eval_modelmatrix <- model.matrix( ~ .-1, eval_df2)
```

Now the *Random Forest* model is ready to be used on the evaluation dataset.

```{r}
# Predicting on evaluation dataset
rfPred <- predict(rfmodel, newdata = eval_modelmatrix)

# Inputting the prediction values into dataset
eval_df$PH <- rfPred
```


Finally, we can write the dataset set with our predictions into an excel spreadsheet.

```{r}
write_xlsx(eval_df, 'StudentEvaluation2.xlsx')
```
